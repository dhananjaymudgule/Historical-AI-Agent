# Historical-AI-Agent


Both `chat_service.py` and `agent_service.py` implement conversational AI using **Gemini LLM** and **LangGraph**, but they take different approaches in structuring the chatbot. Here's a breakdown of their differences:

---

### **1Ô∏è‚É£ Chat Service (`chat_service.py`): LangGraph with Tool Nodes**
#### **How it Works:**
- Uses **LangGraph** explicitly to build a stateful chatbot workflow.
- Maintains **conversation history (`messages` list)** throughout interactions.
- Defines a **state schema (`State`)** using `TypedDict` to ensure structured inputs.
- **Tool invocation**:
  - The chatbot first processes the input message.
  - If the response requires calling a function (tool), it executes it and appends the result to the conversation.
  - Otherwise, it returns the LLM's response.
- Implements **a structured workflow** with:
  - **Nodes** (`assistant` and `tools`).
  - **Edges** (directing flow between nodes).
- Stores **conversation history persistently** in `ChatService`.

#### **Pros:**
‚úÖ **More structured and scalable**: Uses a modular, graph-based approach.  
‚úÖ **Better control over tool execution**: Explicit handling of function calls.  
‚úÖ **Retains chat history** across messages.  
‚úÖ **Easier debugging** with clear function call execution tracking.  

#### **Cons:**
‚ùå Slightly more complex setup due to explicit state handling and LangGraph edges.  
‚ùå Requires manually managing the flow between nodes.

---

### **2Ô∏è‚É£ Agent Service (`agent_service.py`): LangGraph's `create_react_agent`**
#### **How it Works:**
- Uses `create_react_agent`, a **prebuilt LangGraph agent** for reasoning and action execution.
- No explicit conversation state management.
- Defines **tools** (`send_otp`, `verify_otp`) and passes them directly to the agent.
- The **agent internally decides** whether to call a tool or respond normally.
- Uses `SYSTEM_PROMPT` as a **state modifier**.
- Simply **invokes the agent** with the user query (`state={"messages": query}`).

#### **Pros:**
‚úÖ **Easier to implement**: Uses a high-level agent API with minimal manual state handling.  
‚úÖ **Less boilerplate**: No need to define state management explicitly.  
‚úÖ **More natural tool usage**: The agent decides when to call tools.  

#### **Cons:**
‚ùå **Less control over execution flow**: You can't define explicit transitions like in LangGraph.  
‚ùå **No explicit chat history**: You'd need to handle persistence separately.  
‚ùå **Debugging is harder**: No clear function execution tracking.  

---

### **Which One is Better?**
| Feature                  | **Chat Service (`chat_service.py`)** | **Agent Service (`agent_service.py`)** |
|--------------------------|--------------------------------------|--------------------------------------|
| **Ease of Implementation** | ‚ùå More setup required              | ‚úÖ Simpler, fewer lines of code |
| **Scalability**            | ‚úÖ Highly modular & structured       | ‚ùå Less modular, relies on agent internals |
| **Control Over Workflow**  | ‚úÖ Explicit node transitions         | ‚ùå Agent decides actions |
| **Tool Invocation**        | ‚úÖ Manual, explicit execution        | ‚úÖ Automatic but less controllable |
| **Chat History Handling**  | ‚úÖ Persistent across messages       | ‚ùå Needs external storage |
| **Debugging**              | ‚úÖ Clear execution tracking          | ‚ùå Harder to debug |

#### **Best Choice?**
- **For a simple chatbot with tool usage** ‚Üí `agent_service.py` (easier to set up).
- **For a production-ready, scalable chatbot with structured workflow** ‚Üí `chat_service.py` (better control and history handling).

üöÄ **If you're building a serious Gen AI application, `chat_service.py` is the better choice!**